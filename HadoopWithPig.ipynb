{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up a Basic Hadoop Environment for ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll guide you through setting up a basic Hadoop environment for ETL operations using HDFS, YARN, MapReduce, and Apache Pig. We'll execute commands directly from this notebook using cell magics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Operating System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This setup can be performed on Windows, macOS, or Unix-based operating systems. Since Hadoop is designed to run on Unix-like systems, Windows users will need to set up a Unix-like environment using tools like Windows Subsystem for Linux (WSL) or Cygwin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For Windows Users**:\n",
    "\n",
    "**Option 1: Windows Subsystem for Linux (WSL)**\n",
    "\n",
    "WSL allows you to run a Linux distribution directly on Windows. You can install Ubuntu from the Microsoft Store.\n",
    "\n",
    "-   Enable WSL:\n",
    "\n",
    "Open PowerShell as Administrator and run:\n",
    "\n",
    "`Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux`\n",
    "\n",
    "Restart your computer when prompted.\n",
    "\n",
    "- Install Ubuntu:\n",
    "\n",
    "Download and install Ubuntu from the [Microsoft Store](https://apps.microsoft.com/detail/9nblggh4msv6?rtc=1&hl=en-us&gl=US).\n",
    "\n",
    "- Launch Ubuntu and Update Packages:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "sudo apt-get update -V\n",
    "sudo apt-get upgrade -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 2: Cygwin**\n",
    "\n",
    "Cygwin provides a Unix-like environment on Windows.\n",
    "\n",
    "- Download and install Cygwin from www.cygwin.com.\n",
    "- During installation, select the packages required (e.g., OpenSSH, OpenJDK).\n",
    "- Proceed with the instructions in the Cygwin terminal.\n",
    "\n",
    "**Note**: Using WSL is recommended for simplicity and compatibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For MacOS Users:**\n",
    "\n",
    "Most Unix commands are compatible with macOS. You may need to install some packages using Homebrew."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Install Homebrew (if not already installed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Update Homebrew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "brew update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Install Java Development Kit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop requires Java to run. We'll install JDK 8.\n",
    "\n",
    "**Check if Java is Already Installed**\n",
    "\n",
    "Let's verify if Java is already installed on your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "java -version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install JDK 8**\n",
    "If Java is not installed or you need to install JDK 8, follow the instructions below:\n",
    "\n",
    "**For Windows Users (Using WSL or Cygwin):**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Install OpenJDK 8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "sudo apt-get install -y openjdk-8-jdk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Set Java Environment Variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"export JAVA_HOME=$(readlink -f /usr/bin/java | sed 's:/bin/java::')\" >> ~/.bashrc\n",
    "source ~/.bashrc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For macOS Users:**\n",
    "\n",
    "- Install OpenJDK 8 Using Homebrew:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "brew tap homebrew/cask-versions\n",
    "brew install --cask adoptopenjdk8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Set Java Environment Variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"export JAVA_HOME=$(/usr/libexec/java_home -v1.8)\" >> ~/.bash_profile\n",
    "source ~/.bash_profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verify Java Installation**\n",
    "\n",
    "After installation, confirm that Java is correctly installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "java -version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see output similar to:\n",
    "\n",
    "`java version \"1.8.0_xxx\"`\n",
    "\n",
    "`Java(TM) SE Runtime Environment (build 1.8.0_xxx)`\n",
    "\n",
    "`Java HotSpot(TM) 64-Bit Server VM (build xx.x-bxx, mixed mode)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Configure SSH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop uses SSH for communication between nodes, even in a single-node setup. We'll set up passwordless SSH access to `localhost`.\n",
    "\n",
    "First, ensure that SSH is installed:\n",
    "\n",
    "**For Windows Users (Using WSL or Cygwin):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "sudo apt-get install -y openssh-server openssh-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For macOS Users:**\n",
    "\n",
    "SSH is typically pre-installed on macOS. Verify with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ssh -V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate SSH Keys**\n",
    "\n",
    "Generate a new SSH key pair without a passphrase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ssh-keygen -t rsa -P \"\" -f ~/.ssh/id_rsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Configure Authorized Keys**\n",
    "\n",
    "Add your public key to the list of authorized keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\n",
    "chmod 600 ~/.ssh/authorized_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Start SSH Service (If Necessary)**\n",
    "\n",
    "For Windows Users (Using WSL or Cygwin):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "sudo service ssh start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test SSH Connection to localhost**\n",
    "\n",
    "Verify that you can SSH to localhost without a password:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ssh -o StrictHostKeyChecking=no localhost echo \"SSH connection established.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If successful, you should see:\n",
    "\n",
    "`SSH connection established.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Install Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll download and install Hadoop on your system. We'll also set up the necessary environment variables. The instructions are tailored for Windows (using WSL or Cygwin), macOS, and Unix-based systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Download Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll download the latest stable release of Hadoop from the Apache website.\n",
    "\n",
    "**Determine the Latest Stable Version**\n",
    "\n",
    "Please verify the latest version from the Apache Hadoop Releases page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Replace with the latest version number\n",
    "HADOOP_VERSION=3.3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download Hadoop**\n",
    "\n",
    "For All Users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Download Hadoop\n",
    "wget https://downloads.apache.org/hadoop/common/hadoop-$HADOOP_VERSION/hadoop-$HADOOP_VERSION.tar.gz -P ~/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If wget is not installed, you can install it using:\n",
    "\n",
    "- For Ubuntu/WSL/Cygwin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "sudo apt-get install -y wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For macOS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "brew install wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Extract Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unpack the downloaded tarball and move it to a convenient location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Extract Hadoop\n",
    "tar -xzvf ~/hadoop-$HADOOP_VERSION.tar.gz -C ~/\n",
    "\n",
    "# Move Hadoop to /usr/local/hadoop (may require sudo)\n",
    "sudo mv ~/hadoop-$HADOOP_VERSION /usr/local/hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note for Windows Users**: If you encounter permission issues, you might need to adjust the permissions or run the commands with appropriate privileges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Set Environment Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll set up the environment variables required for Hadoop to run properly.\n",
    "\n",
    "**For All Users:**\n",
    "\n",
    "Edit your shell profile file to include Hadoop and Java environment variables.\n",
    "\n",
    "**Determine Your Shell Profile File**\n",
    "\n",
    "- For Bash shell: ~/.bashrc or ~/.bash_profile\n",
    "- For Zsh shell (common on macOS): ~/.zshrc\n",
    "\n",
    "For this guide, we'll assume you're using ~/.bashrc.\n",
    "\n",
    "**Edit the Shell Profile**\n",
    "\n",
    "Append the following lines to your shell profile file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Define shell profile file\n",
    "SHELL_PROFILE=~/.bashrc\n",
    "\n",
    "# Append Hadoop environment variables\n",
    "echo \"\n",
    "# Hadoop Environment Variables\n",
    "export HADOOP_HOME=/usr/local/hadoop\n",
    "export PATH=\\$PATH:\\$HADOOP_HOME/bin:\\$HADOOP_HOME/sbin\n",
    "\" >> \\$SHELL_PROFILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set JAVA_HOME Environment Variable**\n",
    "\n",
    "We need to set JAVA_HOME so that Hadoop knows where Java is installed.\n",
    "\n",
    "**For Ubuntu/WSL/Cygwin Users:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Append JAVA_HOME to shell profile\n",
    "echo \"\n",
    "export JAVA_HOME=\\$(readlink -f /usr/bin/java | sed 's:/bin/java::')\n",
    "\" >> \\$SHELL_PROFILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For macOS Users:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Append JAVA_HOME to shell profile\n",
    "echo \"\n",
    "export JAVA_HOME=\\$(/usr/libexec/java_home -v1.8)\n",
    "\" >> \\$SHELL_PROFILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply the Changes**\n",
    "\n",
    "Reload your shell profile to apply the environment variable changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ~/.bashrc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note for macOS Users**: If you edited ~/.bash_profile or ~/.zshrc, use that file in the source command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Verify Hadoop Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hadoop version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see output similar to:\n",
    "\n",
    "`Hadoop 3.3.1`\n",
    "\n",
    "`Source code repository https://github.com/apache/hadoop -r ...`\n",
    "\n",
    "`Compiled by ... on ...`\n",
    "\n",
    "`Compiled with protoc 3.x.x`\n",
    "\n",
    "`From source with checksum ...`\n",
    "\n",
    "`This command was run using /usr/local/hadoop/share/hadoop/common/hadoop-common-3.3.1.jar`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see this output, Hadoop is installed successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Configure HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Edit `core-site.xml`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `core-site.xml` file contains configuration settings for Hadoop core, such as the default filesystem.\n",
    "\n",
    "**Locate the `core-site.xml` File**\n",
    "\n",
    "The file is located in the Hadoop configuration directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$HADOOP_HOME/etc/hadoop/core-site.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a Backup (Optional)**\n",
    "\n",
    "It's a good practice to create a backup before modifying configuration files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cp $HADOOP_HOME/etc/hadoop/core-site.xml $HADOOP_HOME/etc/hadoop/core-site.xml.backup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Edit `core-site.xml`**\n",
    "\n",
    "We'll add configuration settings to specify the default filesystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat <<EOL > $HADOOP_HOME/etc/hadoop/core-site.xml\n",
    "<?xml version=\"1.0\"?>\n",
    "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
    "<configuration>\n",
    "   <property>\n",
    "       <name>fs.defaultFS</name>\n",
    "       <value>hdfs://localhost:9000</value>\n",
    "   </property>\n",
    "</configuration>\n",
    "EOL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Edit `hdfs-site.xml`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hdfs-site.xml file contains settings specific to HDFS, such as replication factor.\n",
    "\n",
    "**Create a backup (optional)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cp $HADOOP_HOME/etc/hadoop/hdfs-site.xml $HADOOP_HOME/etc/hadoop/hdfs-site.xml.backup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Edit `hdfs-site.xml`**\n",
    "\n",
    "We'll set the replication factor to 1 since we're setting up a single-node cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat <<EOL > $HADOOP_HOME/etc/hadoop/hdfs-site.xml\n",
    "<?xml version=\"1.0\"?>\n",
    "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
    "<configuration>\n",
    "   <property>\n",
    "       <name>dfs.replication</name>\n",
    "       <value>1</value>\n",
    "   </property>\n",
    "</configuration>\n",
    "EOL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Format the NameNode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting HDFS, we need to format the NameNode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs namenode -format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see output indicating that the NameNode has been formatted successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Configure YARN and MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll configure YARN (Yet Another Resource Negotiator) and MapReduce. YARN is Hadoop's cluster resource management system, and MapReduce is the programming model for data processing. We'll edit the necessary configuration files to enable these components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Edit `yarn-site.xml`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `yarn-site.xml` file contains configuration settings for YARN.\n",
    "\n",
    "**Locate the `yarn-site.xml` File**\n",
    "\n",
    "The file is located in the Hadoop configuration directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$HADOOP_HOME/etc/hadoop/yarn-site.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a Backup (Optional)**\n",
    "\n",
    "It's a good practice to create a backup before modifying configuration files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cp $HADOOP_HOME/etc/hadoop/yarn-site.xml $HADOOP_HOME/etc/hadoop/yarn-site.xml.backup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Edit `yarn-site.xml`**\n",
    "\n",
    "We'll add configuration settings to specify the YARN NodeManager auxiliary services and enable the MapReduce shuffle service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat <<EOL > $HADOOP_HOME/etc/hadoop/yarn-site.xml\n",
    "<?xml version=\"1.0\"?>\n",
    "<configuration>\n",
    "    <!-- Site specific YARN configuration properties -->\n",
    "    <property>\n",
    "        <name>yarn.nodemanager.aux-services</name>\n",
    "        <value>mapreduce_shuffle</value>\n",
    "    </property>\n",
    "</configuration>\n",
    "EOL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Edit `mapred-site.xml`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `mapred-site.xml` file contains configuration settings for MapReduce.\n",
    "\n",
    "**Create the `mapred-site.xml` File**\n",
    "\n",
    "If the file does not exist, create it by copying the template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cp $HADOOP_HOME/etc/hadoop/mapred-site.xml.template $HADOOP_HOME/etc/hadoop/mapred-site.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a Backup (Optional)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cp $HADOOP_HOME/etc/hadoop/mapred-site.xml $HADOOP_HOME/etc/hadoop/mapred-site.xml.backup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Edit mapred-site.xml**\n",
    "\n",
    "We'll configure MapReduce to run on YARN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat <<EOL > $HADOOP_HOME/etc/hadoop/mapred-site.xml\n",
    "<?xml version=\"1.0\"?>\n",
    "<configuration>\n",
    "    <!-- Site specific MapReduce configuration properties -->\n",
    "    <property>\n",
    "        <name>mapreduce.framework.name</name>\n",
    "        <value>yarn</value>\n",
    "    </property>\n",
    "</configuration>\n",
    "EOL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Verify Configuration Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can verify the contents of the configuration files to ensure they have been set correctly.\n",
    "\n",
    "**View `yarn-site.xml`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat $HADOOP_HOME/etc/hadoop/yarn-site.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**View `mapred-site.xml`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat $HADOOP_HOME/etc/hadoop/mapred-site.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- `yarn-site.xml`: We specified the yarn.nodemanager.aux-services property and set its value to mapreduce_shuffle. This enables the shuffle service, which is necessary for MapReduce jobs to run properly on YARN.\n",
    "- `mapred-site.xml`: We set the mapreduce.framework.name property to yarn, indicating that MapReduce should use YARN as its resource manager.\n",
    "\n",
    "By configuring these files, we've set up Hadoop to use YARN for resource management and prepared the system to run MapReduce jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Start Hadoop Services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll start the Hadoop services, including HDFS (Hadoop Distributed File System) and YARN (Yet Another Resource Negotiator). We'll also verify that the services are running correctly by accessing their web interfaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Starrt HDFS (NameNode and DataNode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to start the HDFS daemons: the NameNode and DataNode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "start-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: If you encounter a \"command not found\" error, ensure that $HADOOP_HOME/bin and $HADOOP_HOME/sbin are in your PATH. Alternatively, use the full path to the script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "$HADOOP_HOME/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "\n",
    "You should see output indicating that the NameNode and DataNode are starting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "Starting namenodes on [localhost]\n",
    "localhost: starting namenode, logging to /usr/local/hadoop/logs/hadoop-yourusername-namenode-yourhostname.out\n",
    "localhost: starting datanode, logging to /usr/local/hadoop/logs/hadoop-yourusername-datanode-yourhostname.out\n",
    "Starting secondary namenodes [0.0.0.0]\n",
    "0.0.0.0: starting secondarynamenode, logging to /usr/local/hadoop/logs/hadoop-yourusername-secondarynamenode-yourhostname.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Start YARN (ResourceManager and NodeManager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll start the YARN daemons: the ResourceManager and NodeManager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "start-yarn.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, use the full path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "$HADOOP_HOME/sbin/start-yarn.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "\n",
    "You should see output indicating that the ResourceManager and NodeManager are starting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "starting resourcemanager, logging to /usr/local/hadoop/logs/yarn-yourusername-resourcemanager-yourhostname.out\n",
    "localhost: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-yourusername-nodemanager-yourhostname.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Verify Running Services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that the Hadoop services are running by checking the web interfaces provided by HDFS and YARN.\n",
    "\n",
    "**5.3.1 HDFS NameNode Web Interface**\n",
    "\n",
    "- URL: http://localhost:9870\n",
    "\n",
    "Open a web browser and navigate to http://localhost:9870. You should see the HDFS NameNode status page.\n",
    "\n",
    "**5.3.2 YARN ResourceManager Web Interface**\n",
    "- URL: http://localhost:8088\n",
    "Open a web browser and navigate to http://localhost:8088. You should see the YARN ResourceManager status page.\n",
    "\n",
    "**Note**: If you cannot access these pages, ensure that your firewall settings allow connections to these ports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Verify Hadoop Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check if the Hadoop daemons are running by listing Java processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "jps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "\n",
    "You should see output similar to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ResourceManager`\n",
    "\n",
    "`NameNode`\n",
    "\n",
    "`DataNode`\n",
    "\n",
    "`NodeManager`\n",
    "\n",
    "`SecondaryNameNode`\n",
    "\n",
    "`Jps`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Troubleshooting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Common Issues**\n",
    "\n",
    "- Environment Variables Not Set: Ensure that `JAVA_HOME` and `HADOOP_HOME` are set correctly and that `$HADOOP_HOME/bin` and `$HADOOP_HOME/sbin` are in your `PATH`.\n",
    "- SSH Issues: If you encounter SSH connection issues, ensure that passwordless SSH is set up correctly (refer back to Section 1.3).\n",
    "- Permission Denied Errors: Make sure you have the necessary permissions to start Hadoop services. Avoid running Hadoop as the root user; instead, use a regular user account.\n",
    "\n",
    "**Log Files**\n",
    "\n",
    "Check the Hadoop log files for detailed error messages:\n",
    "\n",
    "- HDFS Logs: Located in `$HADOOP_HOME/logs/`, files like `hadoop-yourusername-namenode-yourhostname.out`.\n",
    "- YARN Logs: Located in `$HADOOP_HOME/logs/`, files like `yarn-yourusername-resourcemanager-yourhostname.out`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "\n",
    "- Starting HDFS Services: The `start-dfs.sh` script starts the HDFS daemons, which include the NameNode, DataNode, and SecondaryNameNode.\n",
    "- Starting YARN Services: The `start-yarn.sh` script starts the YARN daemons, including the ResourceManager and NodeManager.\n",
    "- Web Interfaces: Hadoop provides web interfaces to monitor the cluster's status. The NameNode UI shows the status of HDFS, and the ResourceManager UI shows the status of YARN and running applications.\n",
    "- Process Verification: Using the `jps` command, we can list all Java processes to confirm that the necessary Hadoop daemons are running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Run a Sample MapReduce Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll run a sample MapReduce job to ensure that our Hadoop setup is functioning correctly. We'll use the built-in WordCount example that comes with Hadoop. This example will count the frequency of words in a set of input files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Prepare the HDFS Directory Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create directories in HDFS to store our input files and output results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.1 Create a User Directory in HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace yourusername with your actual username if necessary. Alternatively, you can use the whoami command to dynamically get your username."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -mkdir -p /user/$(whoami)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the Directory Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -ls /user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see your username listed in the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Copy Sample Files to HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the Hadoop configuration files as sample input data for the WordCount example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -put $HADOOP_HOME/etc/hadoop/*.xml /user/$(whoami)/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify Files in HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -ls /user/$(whoami)/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a list of .xml files that were copied to your HDFS directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Run the WordCount MapReduce Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll execute the WordCount example using the sample files we just uploaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3.1 Identify the Hadoop MapReduce Examples JAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, find the exact version of the Hadoop MapReduce examples JAR file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command will output the full path to the examples JAR, such as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.1.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3.2 Execute the WordCount Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the WordCount job using the identified JAR file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar wordcount /user/$(whoami) /user/$(whoami)/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "\n",
    "- Input Path: `/user/$(whoami)` (the directory containing your input files)\n",
    "- Output Path: `/user/$(whoami)/output` (the directory where the results will be stored)\n",
    "\n",
    "**Expected Output**\n",
    "\n",
    "The terminal will display logs showing the progress of the MapReduce job. Look for lines indicating that the job has completed successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 View the Output Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the job completes, we can check the output stored in HDFS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4.1 List the Output Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -ls /user/$(whoami)/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see files like part-r-00000, which contain the word count results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4.2 Display the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -cat /user/$(whoami)/output/part-r-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command will output the word counts. You should see something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "Configuration    2\n",
    "Filesystem       3\n",
    "Hadoop           5\n",
    "MapReduce        1\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Verify Job Status on YARN ResourceManager UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also verify that the job ran successfully by checking the YARN web interface.\n",
    "\n",
    "- URL: http://localhost:8088\n",
    "\n",
    "Navigate to this URL in your web browser. Under the \"**Finished Applications**\" section, you should see your WordCount job listed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6 Clean Up the Output Directory (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to run the job again, you need to remove the existing output directory; otherwise, Hadoop will throw an error because the output directory already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -rm -r /user/$(whoami)/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "\n",
    "- **Creating Directories**: We created necessary directories in HDFS to organize our data.\n",
    "\n",
    "- **Uploading Data**: We uploaded sample files to HDFS to serve as input for our MapReduce job.\n",
    "\n",
    "- **Running the Job**: We executed the WordCount example, which processed the input files and produced word counts.\n",
    "\n",
    "- **Viewing Results**: We retrieved the output from HDFS and displayed it.\n",
    "\n",
    "- **Web Interface**: The YARN ResourceManager UI provides a graphical interface to monitor and manage jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Install Apache Pig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll install Apache Pig, a high-level platform for creating programs that run on Hadoop. Pig is designed to simplify the processing and analysis of large datasets. We'll download and install Pig, set the necessary environment variables, and ensure it's correctly configured to work with our Hadoop setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Download Apache Pig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll download the latest stable version of Apache Pig from the Apache website.\n",
    "\n",
    "**Determine the Latest Stable Version**\n",
    "\n",
    "Please check the Apache Pig Releases page to confirm the latest version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Replace with the latest Pig version number if necessary\n",
    "PIG_VERSION=0.17.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download Apache Pig**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Download Apache Pig\n",
    "wget https://downloads.apache.org/pig/pig-$PIG_VERSION/pig-$PIG_VERSION.tar.gz -P ~/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: If wget is not installed, you can install it using:\n",
    "\n",
    "- For Ubuntu/WSL/Cygwin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "sudo apt-get install -y wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For macOS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "brew install wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Extract Apache Pig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unpack the downloaded tarball and move it to a suitable location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Extract Pig\n",
    "tar -xzvf ~/pig-$PIG_VERSION.tar.gz -C ~/\n",
    "\n",
    "# Move Pig to /usr/local/pig (may require sudo)\n",
    "sudo mv ~/pig-$PIG_VERSION /usr/local/pig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note for Windows Users**: If you encounter permission issues, you might need to adjust permissions or run the commands with administrative privileges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Set Up Environment Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to set up environment variables so that the system recognizes Pig commands.\n",
    "\n",
    "**Edit Your Shell Profile**\n",
    "\n",
    "Append the following lines to your shell profile file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Define shell profile file\n",
    "SHELL_PROFILE=~/.bashrc  # Use ~/.bash_profile or ~/.zshrc if appropriate\n",
    "\n",
    "# Append Pig environment variables\n",
    "echo \"\n",
    "# Pig Environment Variables\n",
    "export PIG_HOME=/usr/local/pig\n",
    "export PATH=\\$PATH:\\$PIG_HOME/bin\n",
    "\" >> \\$SHELL_PROFILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply the Changes**\n",
    "\n",
    "Reload your shell profile to apply the environment variable changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ~/.bashrc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note for macOS Users**: If you edited ~/.bash_profile or ~/.zshrc, replace ~/.bashrc with the appropriate file in the source command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Verify Apache Pig Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm that Pig is installed and configured correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "pig -version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see output similar to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "Apache Pig version 0.17.0 (r1744791) compiled Mar 3 2016, 14:42:23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Configure Pig (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Pig runs in **MapReduce mode**, which requires a running Hadoop cluster. Since we have Hadoop configured, no additional setup is needed for Pig to run in this mode.\n",
    "\n",
    "If you wish to run Pig in **local mode** (without Hadoop), you can specify the -x local option when running Pig commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "\n",
    "- **Downloading Pig**: We downloaded the specified version of Apache Pig to ensure compatibility with our Hadoop installation.\n",
    "- **Setting Environment Variables**: By updating PIG_HOME and PATH, we make Pig commands accessible from any directory.\n",
    "- **Verification**: Running pig -version checks that Pig is installed and can interact with Hadoop.\n",
    "- **Configuration**: Pig is set to run in MapReduce mode by default, which is suitable for our setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Run an ETL Process Using Apache Pig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll perform an ETL (Extract, Transform, Load) operation using Apache Pig. We'll use Pig Latin scripts to process data stored in HDFS. This exercise will demonstrate how Pig simplifies complex data transformations and makes it easier to work with large datasets on Hadoop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Prepare Sample Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a sample data file and upload it to HDFS to serve as the input for our Pig script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1.1 Create a Sample Data File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a file named student_data.txt with the following content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat <<EOL > ~/student_data.txt\n",
    "1,John,Doe,85\n",
    "2,Jane,Smith,92\n",
    "3,Bob,Johnson,76\n",
    "4,Alice,Williams,89\n",
    "5,Tom,Brown,95\n",
    "EOL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "\n",
    "Each line represents a student's record with the following fields:\n",
    "\n",
    "- `id`: Student ID (integer)\n",
    "- `first_name`: First name (string)\n",
    "- `last_name`: Last name (string)\n",
    "- `score`: Exam score (integer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1.2 Upload the Data File to HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll copy the student_data.txt file from the local filesystem to HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -mkdir -p /user/$(whoami)/pig_data\n",
    "hdfs dfs -put ~/student_data.txt /user/$(whoami)/pig_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the file in HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -ls /user/$(whoami)/pig_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Write a Pig Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll write a Pig script named etl.pig to perform the ETL operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2.1 Create the Pig Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat <<EOL > ~/etl.pig\n",
    "-- Load data from HDFS\n",
    "student_data = LOAD '/user/$(whoami)/pig_data/student_data.txt' USING PigStorage(',')\n",
    "                 AS (id:int, first_name:chararray, last_name:chararray, score:int);\n",
    "\n",
    "-- Filter records with score greater than 80\n",
    "high_scores = FILTER student_data BY score > 80;\n",
    "\n",
    "-- Group data by score\n",
    "grouped_data = GROUP high_scores BY score;\n",
    "\n",
    "-- Calculate average score (optional step)\n",
    "avg_score = FOREACH grouped_data GENERATE group AS score, COUNT(high_scores) AS count;\n",
    "\n",
    "-- Store the result back to HDFS\n",
    "STORE high_scores INTO '/user/$(whoami)/pig_output/high_scores' USING PigStorage(',');\n",
    "\n",
    "-- Store the average score (optional)\n",
    "STORE avg_score INTO '/user/$(whoami)/pig_output/avg_score' USING PigStorage(',');\n",
    "EOL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "\n",
    "- **Load** Statement: Reads data from HDFS using PigStorage with a comma as the delimiter.\n",
    "- **Filter** Statement: Filters records where score > 80.\n",
    "- **Group** Statement: Groups the filtered data by score.\n",
    "- **Foreach** Statement: Calculates the count of students for each score (optional step).\n",
    "- **Store** Statements: Saves the filtered data and average scores back to HDFS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2.2 Review the Pig Script (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat ~/etl.pig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Execute the Pig Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll run the Pig script using the pig command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "pig ~/etl.pig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Ensure that Hadoop services are running before executing the Pig script. If not, refer back to Section 5 to start Hadoop services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "\n",
    "The terminal will display logs showing the progress of the Pig script execution. Look for lines indicating successful completion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "Success!\n",
    "Job Stats (time in seconds):\n",
    "JobId   Maps   Reduces  MaxMapTime  MinMapTime  AvgMapTime  MaxReduceTime  MinReduceTime  AvgReduceTime  Alias        Feature  Outputs\n",
    "job_...     1        1         ...         ...         ...            ...            ...            ...  student_data  MAP_ONLY  ...\n",
    "job_...     1        1         ...         ...         ...            ...            ...            ...  high_scores   HADOOP   ...\n",
    "\n",
    "Input(s):\n",
    "Successfully read ... records\n",
    "\n",
    "Output(s):\n",
    "Successfully stored ... records\n",
    "\n",
    "Counters:\n",
    "Total records written : ...\n",
    "Total bytes written : ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Verify the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll check the output stored in HDFS to ensure our ETL process worked as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.1 List the Output Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -ls /user/$(whoami)/pig_output/high_scores/\n",
    "hdfs dfs -ls /user/$(whoami)/pig_output/avg_score/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see part-m-00000 files in each directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.2 Display the High Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -cat /user/$(whoami)/pig_output/high_scores/part-m-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected Output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "2,Jane,Smith,92\n",
    "4,Alice,Williams,89\n",
    "5,Tom,Brown,95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.3 Display the Average Scores (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -cat /user/$(whoami)/pig_output/avg_score/part-r-00000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected Output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "89,1\n",
    "92,1\n",
    "95,1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "\n",
    "Each line represents a score and the count of students who achieved it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5 Clean Up (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to rerun the Pig script, you need to remove the existing output directories to avoid errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -rm -r /user/$(whoami)/pig_output/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "\n",
    "- **Data Preparation**: We created a sample dataset representing student records and uploaded it to HDFS.\n",
    "- **Pig Script**: We wrote a Pig script to load the data, filter students with scores greater than 80, group the data, and calculate counts.\n",
    "- **Execution**: We ran the Pig script, which executed MapReduce jobs under the hood.\n",
    "- **Verification**: We retrieved and displayed the output from HDFS to confirm that the ETL process was successful.\n",
    "\n",
    "**Key Concepts**:\n",
    "\n",
    "- **Pig Latin**: The scripting language used by Apache Pig, which simplifies writing data transformation operations.\n",
    "- **ETL Process**: Extracting data from a source, transforming it according to business logic, and loading it into a destination.\n",
    "- **MapReduce Jobs**: Pig scripts are internally converted into MapReduce jobs for execution on Hadoop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: Cleanup and Shutdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this final section, we'll cover how to properly shut down the Hadoop services and clean up any temporary files or directories created during this exercise. Regular cleanup helps maintain system performance and frees up resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Stop Hadoop Services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To gracefully shut down the Hadoop services, we'll stop both HDFS and YARN daemons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.1.1 Stop YARN Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "stop-yarn.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, if the command is not found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "$HADOOP_HOME/sbin/stop-yarn.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected Output\n",
    "\n",
    "You should see messages indicating that the ResourceManager and NodeManager are stopping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "stopping resourcemanager\n",
    "localhost: stopping nodemanager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.1.2 Stop HDFS Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "stop-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "$HADOOP_HOME/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "\n",
    "You should see messages indicating that the NameNode and DataNode are stopping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "Stopping namenodes on [localhost]\n",
    "localhost: stopping namenode\n",
    "localhost: stopping datanode\n",
    "Stopping secondary namenodes [0.0.0.0]\n",
    "0.0.0.0: stopping secondarynamenode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Verify that Services Have Stopped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check that the Hadoop daemons are no longer running by listing Java processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "jps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "\n",
    "You should only see the Jps process listed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "Jps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Clean Up Temp Files (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During Hadoop's operation, temporary files and directories are created. You may choose to clean these up to free disk space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.3.1 Remove HDFS Data Directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to completely remove all data stored in HDFS (including the NameNode and DataNode data), you can delete the Hadoop data directories. By default, these are located in /tmp/hadoop-$(whoami).\n",
    "\n",
    "**Warning**: This will delete all data in HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf /tmp/hadoop-$(whoami)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.3.2 Remove HDFS Files and Directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you only want to remove the files and directories created in HDFS during this exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -rm -r /user/$(whoami)/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4 Remove Sample Data Files (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to remove the sample data files from your local filesystem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm ~/student_data.txt\n",
    "rm ~/etl.pig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.5 Reset Environment Variables (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to remove the environment variables set during this setup:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.5.1 Edit Your Shell Profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open your shell profile file (e.g., ~/.bashrc, ~/.bash_profile, or ~/.zshrc) in a text editor and remove the lines related to HADOOP_HOME, PIG_HOME, and their additions to the PATH.\n",
    "\n",
    "Alternatively, you can use the following commands to remove them automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "sed -i '/# Hadoop Environment Variables/,+2d' ~/.bashrc\n",
    "sed -i '/# Pig Environment Variables/,+2d' ~/.bashrc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The sed command may differ between systems, and the above command works for GNU sed. On macOS, you might need to use sed -i '' instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.6 Restart Your Shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After making changes to your shell profile, restart your terminal or reload the shell configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ~/.bashrc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.7 Additional Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you installed any software specifically for this exercise and wish to remove it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.7.1 Remove Hadoop and Pig Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "sudo rm -rf /usr/local/hadoop\n",
    "sudo rm -rf /usr/local/pig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.7.2 Remove Downloaded Tarballs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm ~/hadoop-*.tar.gz\n",
    "rm ~/pig-*.tar.gz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "\n",
    "- **Stopping Services**: It's important to properly stop Hadoop services to ensure all processes terminate gracefully.\n",
    "- **Cleaning Up**: Removing temporary files and directories helps maintain system health and frees up disk space.\n",
    "- **Environment Variables**: Resetting environment variables cleans up your shell environment if you no longer need Hadoop and Pig commands accessible globally.\n",
    "- **Data Preservation**: Be cautious when deleting data directories to avoid accidental loss of important data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You've completed setting up a basic Hadoop environment and performed ETL operations using Apache Pig. You've learned how to:\n",
    "\n",
    "- Install and configure Hadoop components: HDFS, YARN, and MapReduce.\n",
    "- Start and stop Hadoop services.\n",
    "- Run a sample MapReduce job.\n",
    "- Install and use Apache Pig for data processing.\n",
    "- Clean up your environment after use.\n",
    "\n",
    "This exercise has provided hands-on experience with Hadoop's ecosystem, and you're now better equipped to explore more advanced features and large-scale data processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Steps:\n",
    "\n",
    "- Explore More Pig Scripts: Try modifying the Pig script to perform different transformations or analyses on your data.\n",
    "- Learn Hive: Consider setting up Apache Hive for SQL-like querying of large datasets on Hadoop.\n",
    "- Set Up a Multi-Node Cluster: If you're interested in scaling up, explore setting up a multi-node Hadoop cluster to handle larger workloads."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
