{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up a Basic Hadoop Environment for ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll guide you through setting up a basic Hadoop environment for ETL operations using HDFS, YARN, MapReduce, and Apache Pig. We'll execute commands directly from this notebook using cell magics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Operating System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This setup can be performed on Windows, macOS, or Unix-based operating systems. Since Hadoop is designed to run on Unix-like systems, Windows users will need to set up a Unix-like environment using tools like Windows Subsystem for Linux (WSL) or Cygwin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For Windows Users**:\n",
    "\n",
    "**Option 1: Windows Subsystem for Linux (WSL)**\n",
    "\n",
    "WSL allows you to run a Linux distribution directly on Windows. You can install Ubuntu from the Microsoft Store.\n",
    "\n",
    "-   Enable WSL:\n",
    "\n",
    "Open PowerShell as Administrator and run:\n",
    "\n",
    "`Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux`\n",
    "\n",
    "Restart your computer when prompted.\n",
    "\n",
    "- Install Ubuntu:\n",
    "\n",
    "Download and install Ubuntu from the [Microsoft Store](https://apps.microsoft.com/detail/9nblggh4msv6?rtc=1&hl=en-us&gl=US).\n",
    "\n",
    "- Launch Ubuntu and Update Packages:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "sudo apt-get update -V\n",
    "sudo apt-get upgrade -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 2: Cygwin**\n",
    "\n",
    "Cygwin provides a Unix-like environment on Windows.\n",
    "\n",
    "- Download and install Cygwin from www.cygwin.com.\n",
    "- During installation, select the packages required (e.g., OpenSSH, OpenJDK).\n",
    "- Proceed with the instructions in the Cygwin terminal.\n",
    "\n",
    "**Note**: Using WSL is recommended for simplicity and compatibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For MacOS Users:**\n",
    "\n",
    "Most Unix commands are compatible with macOS. You may need to install some packages using Homebrew."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Install Homebrew (if not already installed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Update Homebrew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "brew update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Install Java Development Kit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop requires Java to run. We'll install JDK 8.\n",
    "\n",
    "**Check if Java is Already Installed**\n",
    "\n",
    "Let's verify if Java is already installed on your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "java -version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install JDK 8**\n",
    "If Java is not installed or you need to install JDK 8, follow the instructions below:\n",
    "\n",
    "**For Windows Users (Using WSL or Cygwin):**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Install OpenJDK 8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "sudo apt-get install -y openjdk-8-jdk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Set Java Environment Variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"export JAVA_HOME=$(readlink -f /usr/bin/java | sed 's:/bin/java::')\" >> ~/.bashrc\n",
    "source ~/.bashrc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For macOS Users:**\n",
    "\n",
    "- Install OpenJDK 8 Using Homebrew:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "brew tap homebrew/cask-versions\n",
    "brew install --cask adoptopenjdk8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Set Java Environment Variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"export JAVA_HOME=$(/usr/libexec/java_home -v1.8)\" >> ~/.bash_profile\n",
    "source ~/.bash_profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verify Java Installation**\n",
    "\n",
    "After installation, confirm that Java is correctly installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "java -version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see output similar to:\n",
    "\n",
    "`java version \"1.8.0_xxx\"`\n",
    "\n",
    "`Java(TM) SE Runtime Environment (build 1.8.0_xxx)`\n",
    "\n",
    "`Java HotSpot(TM) 64-Bit Server VM (build xx.x-bxx, mixed mode)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Configure SSH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop uses SSH for communication between nodes, even in a single-node setup. We'll set up passwordless SSH access to `localhost`.\n",
    "\n",
    "First, ensure that SSH is installed:\n",
    "\n",
    "**For Windows Users (Using WSL or Cygwin):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "sudo apt-get install -y openssh-server openssh-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For macOS Users:**\n",
    "\n",
    "SSH is typically pre-installed on macOS. Verify with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ssh -V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate SSH Keys**\n",
    "\n",
    "Generate a new SSH key pair without a passphrase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ssh-keygen -t rsa -P \"\" -f ~/.ssh/id_rsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Configure Authorized Keys**\n",
    "\n",
    "Add your public key to the list of authorized keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\n",
    "chmod 600 ~/.ssh/authorized_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Start SSH Service (If Necessary)**\n",
    "\n",
    "For Windows Users (Using WSL or Cygwin):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "sudo service ssh start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test SSH Connection to localhost**\n",
    "\n",
    "Verify that you can SSH to localhost without a password:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ssh -o StrictHostKeyChecking=no localhost echo \"SSH connection established.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If successful, you should see:\n",
    "\n",
    "`SSH connection established.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Install Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll download and install Hadoop on your system. We'll also set up the necessary environment variables. The instructions are tailored for Windows (using WSL or Cygwin), macOS, and Unix-based systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Download Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll download the latest stable release of Hadoop from the Apache website.\n",
    "\n",
    "**Determine the Latest Stable Version**\n",
    "\n",
    "Please verify the latest version from the Apache Hadoop Releases page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Replace with the latest version number\n",
    "HADOOP_VERSION=3.3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download Hadoop**\n",
    "\n",
    "For All Users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Download Hadoop\n",
    "wget https://downloads.apache.org/hadoop/common/hadoop-$HADOOP_VERSION/hadoop-$HADOOP_VERSION.tar.gz -P ~/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If wget is not installed, you can install it using:\n",
    "\n",
    "- For Ubuntu/WSL/Cygwin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "sudo apt-get install -y wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For macOS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "brew install wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Extract Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unpack the downloaded tarball and move it to a convenient location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Extract Hadoop\n",
    "tar -xzvf ~/hadoop-$HADOOP_VERSION.tar.gz -C ~/\n",
    "\n",
    "# Move Hadoop to /usr/local/hadoop (may require sudo)\n",
    "sudo mv ~/hadoop-$HADOOP_VERSION /usr/local/hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note for Windows Users**: If you encounter permission issues, you might need to adjust the permissions or run the commands with appropriate privileges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Set Environment Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll set up the environment variables required for Hadoop to run properly.\n",
    "\n",
    "**For All Users:**\n",
    "\n",
    "Edit your shell profile file to include Hadoop and Java environment variables.\n",
    "\n",
    "**Determine Your Shell Profile File**\n",
    "\n",
    "- For Bash shell: ~/.bashrc or ~/.bash_profile\n",
    "- For Zsh shell (common on macOS): ~/.zshrc\n",
    "\n",
    "For this guide, we'll assume you're using ~/.bashrc.\n",
    "\n",
    "**Edit the Shell Profile**\n",
    "\n",
    "Append the following lines to your shell profile file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Define shell profile file\n",
    "SHELL_PROFILE=~/.bashrc\n",
    "\n",
    "# Append Hadoop environment variables\n",
    "echo \"\n",
    "# Hadoop Environment Variables\n",
    "export HADOOP_HOME=/usr/local/hadoop\n",
    "export PATH=\\$PATH:\\$HADOOP_HOME/bin:\\$HADOOP_HOME/sbin\n",
    "\" >> \\$SHELL_PROFILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set JAVA_HOME Environment Variable**\n",
    "\n",
    "We need to set JAVA_HOME so that Hadoop knows where Java is installed.\n",
    "\n",
    "**For Ubuntu/WSL/Cygwin Users:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Append JAVA_HOME to shell profile\n",
    "echo \"\n",
    "export JAVA_HOME=\\$(readlink -f /usr/bin/java | sed 's:/bin/java::')\n",
    "\" >> \\$SHELL_PROFILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For macOS Users:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Append JAVA_HOME to shell profile\n",
    "echo \"\n",
    "export JAVA_HOME=\\$(/usr/libexec/java_home -v1.8)\n",
    "\" >> \\$SHELL_PROFILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply the Changes**\n",
    "\n",
    "Reload your shell profile to apply the environment variable changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ~/.bashrc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note for macOS Users**: If you edited ~/.bash_profile or ~/.zshrc, use that file in the source command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Verify Hadoop Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hadoop version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see output similar to:\n",
    "\n",
    "`Hadoop 3.3.1`\n",
    "\n",
    "`Source code repository https://github.com/apache/hadoop -r ...`\n",
    "\n",
    "`Compiled by ... on ...`\n",
    "\n",
    "`Compiled with protoc 3.x.x`\n",
    "\n",
    "`From source with checksum ...`\n",
    "\n",
    "`This command was run using /usr/local/hadoop/share/hadoop/common/hadoop-common-3.3.1.jar`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see this output, Hadoop is installed successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Configure HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Edit `core-site.xml`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `core-site.xml` file contains configuration settings for Hadoop core, such as the default filesystem.\n",
    "\n",
    "**Locate the `core-site.xml` File**\n",
    "\n",
    "The file is located in the Hadoop configuration directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$HADOOP_HOME/etc/hadoop/core-site.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a Backup (Optional)**\n",
    "\n",
    "It's a good practice to create a backup before modifying configuration files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cp $HADOOP_HOME/etc/hadoop/core-site.xml $HADOOP_HOME/etc/hadoop/core-site.xml.backup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Edit `core-site.xml`**\n",
    "\n",
    "We'll add configuration settings to specify the default filesystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat <<EOL > $HADOOP_HOME/etc/hadoop/core-site.xml\n",
    "<?xml version=\"1.0\"?>\n",
    "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
    "<configuration>\n",
    "   <property>\n",
    "       <name>fs.defaultFS</name>\n",
    "       <value>hdfs://localhost:9000</value>\n",
    "   </property>\n",
    "</configuration>\n",
    "EOL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Edit `hdfs-site.xml`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hdfs-site.xml file contains settings specific to HDFS, such as replication factor.\n",
    "\n",
    "**Create a backup (optional)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cp $HADOOP_HOME/etc/hadoop/hdfs-site.xml $HADOOP_HOME/etc/hadoop/hdfs-site.xml.backup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Edit `hdfs-site.xml`**\n",
    "\n",
    "We'll set the replication factor to 1 since we're setting up a single-node cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat <<EOL > $HADOOP_HOME/etc/hadoop/hdfs-site.xml\n",
    "<?xml version=\"1.0\"?>\n",
    "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
    "<configuration>\n",
    "   <property>\n",
    "       <name>dfs.replication</name>\n",
    "       <value>1</value>\n",
    "   </property>\n",
    "</configuration>\n",
    "EOL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Format the NameNode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting HDFS, we need to format the NameNode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs namenode -format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see output indicating that the NameNode has been formatted successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Configure YARN and MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll configure YARN (Yet Another Resource Negotiator) and MapReduce. YARN is Hadoop's cluster resource management system, and MapReduce is the programming model for data processing. We'll edit the necessary configuration files to enable these components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Edit `yarn-site.xml`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `yarn-site.xml` file contains configuration settings for YARN.\n",
    "\n",
    "**Locate the `yarn-site.xml` File**\n",
    "\n",
    "The file is located in the Hadoop configuration directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$HADOOP_HOME/etc/hadoop/yarn-site.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a Backup (Optional)**\n",
    "\n",
    "It's a good practice to create a backup before modifying configuration files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cp $HADOOP_HOME/etc/hadoop/yarn-site.xml $HADOOP_HOME/etc/hadoop/yarn-site.xml.backup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Edit `yarn-site.xml`**\n",
    "\n",
    "We'll add configuration settings to specify the YARN NodeManager auxiliary services and enable the MapReduce shuffle service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat <<EOL > $HADOOP_HOME/etc/hadoop/yarn-site.xml\n",
    "<?xml version=\"1.0\"?>\n",
    "<configuration>\n",
    "    <!-- Site specific YARN configuration properties -->\n",
    "    <property>\n",
    "        <name>yarn.nodemanager.aux-services</name>\n",
    "        <value>mapreduce_shuffle</value>\n",
    "    </property>\n",
    "</configuration>\n",
    "EOL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Edit `mapred-site.xml`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `mapred-site.xml` file contains configuration settings for MapReduce.\n",
    "\n",
    "**Create the `mapred-site.xml` File**\n",
    "\n",
    "If the file does not exist, create it by copying the template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cp $HADOOP_HOME/etc/hadoop/mapred-site.xml.template $HADOOP_HOME/etc/hadoop/mapred-site.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a Backup (Optional)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cp $HADOOP_HOME/etc/hadoop/mapred-site.xml $HADOOP_HOME/etc/hadoop/mapred-site.xml.backup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Edit mapred-site.xml**\n",
    "\n",
    "We'll configure MapReduce to run on YARN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat <<EOL > $HADOOP_HOME/etc/hadoop/mapred-site.xml\n",
    "<?xml version=\"1.0\"?>\n",
    "<configuration>\n",
    "    <!-- Site specific MapReduce configuration properties -->\n",
    "    <property>\n",
    "        <name>mapreduce.framework.name</name>\n",
    "        <value>yarn</value>\n",
    "    </property>\n",
    "</configuration>\n",
    "EOL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Verify Configuration Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can verify the contents of the configuration files to ensure they have been set correctly.\n",
    "\n",
    "**View `yarn-site.xml`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat $HADOOP_HOME/etc/hadoop/yarn-site.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**View `mapred-site.xml`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat $HADOOP_HOME/etc/hadoop/mapred-site.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- `yarn-site.xml`: We specified the yarn.nodemanager.aux-services property and set its value to mapreduce_shuffle. This enables the shuffle service, which is necessary for MapReduce jobs to run properly on YARN.\n",
    "- `mapred-site.xml`: We set the mapreduce.framework.name property to yarn, indicating that MapReduce should use YARN as its resource manager.\n",
    "\n",
    "By configuring these files, we've set up Hadoop to use YARN for resource management and prepared the system to run MapReduce jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Start Hadoop Services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll start the Hadoop services, including HDFS (Hadoop Distributed File System) and YARN (Yet Another Resource Negotiator). We'll also verify that the services are running correctly by accessing their web interfaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Starrt HDFS (NameNode and DataNode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to start the HDFS daemons: the NameNode and DataNode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "start-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: If you encounter a \"command not found\" error, ensure that $HADOOP_HOME/bin and $HADOOP_HOME/sbin are in your PATH. Alternatively, use the full path to the script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "$HADOOP_HOME/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "\n",
    "You should see output indicating that the NameNode and DataNode are starting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "Starting namenodes on [localhost]\n",
    "localhost: starting namenode, logging to /usr/local/hadoop/logs/hadoop-yourusername-namenode-yourhostname.out\n",
    "localhost: starting datanode, logging to /usr/local/hadoop/logs/hadoop-yourusername-datanode-yourhostname.out\n",
    "Starting secondary namenodes [0.0.0.0]\n",
    "0.0.0.0: starting secondarynamenode, logging to /usr/local/hadoop/logs/hadoop-yourusername-secondarynamenode-yourhostname.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Start YARN (ResourceManager and NodeManager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll start the YARN daemons: the ResourceManager and NodeManager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "start-yarn.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, use the full path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "$HADOOP_HOME/sbin/start-yarn.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "\n",
    "You should see output indicating that the ResourceManager and NodeManager are starting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "starting resourcemanager, logging to /usr/local/hadoop/logs/yarn-yourusername-resourcemanager-yourhostname.out\n",
    "localhost: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-yourusername-nodemanager-yourhostname.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Verify Running Services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that the Hadoop services are running by checking the web interfaces provided by HDFS and YARN.\n",
    "\n",
    "**5.3.1 HDFS NameNode Web Interface**\n",
    "\n",
    "- URL: http://localhost:9870\n",
    "\n",
    "Open a web browser and navigate to http://localhost:9870. You should see the HDFS NameNode status page.\n",
    "\n",
    "**5.3.2 YARN ResourceManager Web Interface**\n",
    "- URL: http://localhost:8088\n",
    "Open a web browser and navigate to http://localhost:8088. You should see the YARN ResourceManager status page.\n",
    "\n",
    "**Note**: If you cannot access these pages, ensure that your firewall settings allow connections to these ports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Verify Hadoop Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check if the Hadoop daemons are running by listing Java processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "jps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "\n",
    "You should see output similar to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ResourceManager`\n",
    "\n",
    "`NameNode`\n",
    "\n",
    "`DataNode`\n",
    "\n",
    "`NodeManager`\n",
    "\n",
    "`SecondaryNameNode`\n",
    "\n",
    "`Jps`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Troubleshooting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Common Issues**\n",
    "\n",
    "- Environment Variables Not Set: Ensure that `JAVA_HOME` and `HADOOP_HOME` are set correctly and that `$HADOOP_HOME/bin` and `$HADOOP_HOME/sbin` are in your `PATH`.\n",
    "- SSH Issues: If you encounter SSH connection issues, ensure that passwordless SSH is set up correctly (refer back to Section 1.3).\n",
    "- Permission Denied Errors: Make sure you have the necessary permissions to start Hadoop services. Avoid running Hadoop as the root user; instead, use a regular user account.\n",
    "\n",
    "**Log Files**\n",
    "\n",
    "Check the Hadoop log files for detailed error messages:\n",
    "\n",
    "- HDFS Logs: Located in `$HADOOP_HOME/logs/`, files like `hadoop-yourusername-namenode-yourhostname.out`.\n",
    "- YARN Logs: Located in `$HADOOP_HOME/logs/`, files like `yarn-yourusername-resourcemanager-yourhostname.out`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "\n",
    "- Starting HDFS Services: The `start-dfs.sh` script starts the HDFS daemons, which include the NameNode, DataNode, and SecondaryNameNode.\n",
    "- Starting YARN Services: The `start-yarn.sh` script starts the YARN daemons, including the ResourceManager and NodeManager.\n",
    "- Web Interfaces: Hadoop provides web interfaces to monitor the cluster's status. The NameNode UI shows the status of HDFS, and the ResourceManager UI shows the status of YARN and running applications.\n",
    "- Process Verification: Using the `jps` command, we can list all Java processes to confirm that the necessary Hadoop daemons are running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Run a Sample MapReduce Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll run a sample MapReduce job to ensure that our Hadoop setup is functioning correctly. We'll use the built-in WordCount example that comes with Hadoop. This example will count the frequency of words in a set of input files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Prepare the HDFS Directory Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create directories in HDFS to store our input files and output results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.1 Create a User Directory in HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace yourusername with your actual username if necessary. Alternatively, you can use the whoami command to dynamically get your username."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -mkdir -p /user/$(whoami)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the Directory Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -ls /user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see your username listed in the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Copy Sample Files to HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the Hadoop configuration files as sample input data for the WordCount example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -put $HADOOP_HOME/etc/hadoop/*.xml /user/$(whoami)/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify Files in HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -ls /user/$(whoami)/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a list of .xml files that were copied to your HDFS directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Run the WordCount MapReduce Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll execute the WordCount example using the sample files we just uploaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3.1 Identify the Hadoop MapReduce Examples JAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, find the exact version of the Hadoop MapReduce examples JAR file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command will output the full path to the examples JAR, such as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.1.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3.2 Execute the WordCount Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the WordCount job using the identified JAR file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar wordcount /user/$(whoami) /user/$(whoami)/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "\n",
    "- Input Path: `/user/$(whoami)` (the directory containing your input files)\n",
    "- Output Path: `/user/$(whoami)/output` (the directory where the results will be stored)\n",
    "\n",
    "**Expected Output**\n",
    "\n",
    "The terminal will display logs showing the progress of the MapReduce job. Look for lines indicating that the job has completed successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 View the Output Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the job completes, we can check the output stored in HDFS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4.1 List the Output Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -ls /user/$(whoami)/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see files like part-r-00000, which contain the word count results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4.2 Display the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -cat /user/$(whoami)/output/part-r-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command will output the word counts. You should see something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "Configuration    2\n",
    "Filesystem       3\n",
    "Hadoop           5\n",
    "MapReduce        1\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Verify Job Status on YARN ResourceManager UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also verify that the job ran successfully by checking the YARN web interface.\n",
    "\n",
    "- URL: http://localhost:8088\n",
    "\n",
    "Navigate to this URL in your web browser. Under the \"**Finished Applications**\" section, you should see your WordCount job listed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6 Clean Up the Output Directory (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to run the job again, you need to remove the existing output directory; otherwise, Hadoop will throw an error because the output directory already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -rm -r /user/$(whoami)/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "\n",
    "- **Creating Directories**: We created necessary directories in HDFS to organize our data.\n",
    "\n",
    "- **Uploading Data**: We uploaded sample files to HDFS to serve as input for our MapReduce job.\n",
    "\n",
    "- **Running the Job**: We executed the WordCount example, which processed the input files and produced word counts.\n",
    "\n",
    "- **Viewing Results**: We retrieved the output from HDFS and displayed it.\n",
    "\n",
    "- **Web Interface**: The YARN ResourceManager UI provides a graphical interface to monitor and manage jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Install Apache Hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll install Apache Hive, a data warehouse software that facilitates reading, writing, and managing large datasets residing in distributed storage using SQL. Hive allows you to write SQL-like queries to process and analyze data stored in HDFS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 7.1 Install Apache Hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll download the latest stable version of Apache Hive from the Apache website.\n",
    "\n",
    "**Determine the Latest Stable Version**\n",
    "\n",
    "Please check the Apache Hive Releases page to confirm the latest version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Replace with the latest Hive version number if necessary\n",
    "HIVE_VERSION=3.1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download Apache Hive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Download Apache Hive\n",
    "wget https://downloads.apache.org/hive/hive-$HIVE_VERSION/apache-hive-$HIVE_VERSION-bin.tar.gz -P ~/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: If wget is not installed, you can install it using:\n",
    "\n",
    "- For Ubuntu/WSL/Cygwin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "sudo apt-get install -y wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For macOS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "brew install wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 7.2 Extract Apache Hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unpack the downloaded tarball and move it to a suitable location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Extract Hive\n",
    "tar -xzvf ~/apache-hive-$HIVE_VERSION-bin.tar.gz -C ~/\n",
    "\n",
    "# Move Hive to /usr/local/hive (may require sudo)\n",
    "sudo mv ~/apache-hive-$HIVE_VERSION-bin /usr/local/hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note for Windows Users**: If you encounter permission issues, you might need to adjust permissions or run the commands with administrative privileges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Set Environment Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to set up environment variables so that the system recognizes Hive commands.\n",
    "\n",
    "**Edit Your Shell Profile**\n",
    "\n",
    "Append the following lines to your shell profile file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Define shell profile file\n",
    "SHELL_PROFILE=~/.bashrc  # Use ~/.bash_profile or ~/.zshrc if appropriate\n",
    "\n",
    "# Append Hive environment variables\n",
    "echo \"\n",
    "# Hive Environment Variables\n",
    "export HIVE_HOME=/usr/local/hive\n",
    "export PATH=\\$PATH:\\$HIVE_HOME/bin\n",
    "\" >> \\$SHELL_PROFILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply the Changes**\n",
    "\n",
    "Reload your shell profile to apply the environment variable changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ~/.bashrc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note for macOS Users**: If you edited ~/.bash_profile or ~/.zshrc, replace ~/.bashrc with the appropriate file in the source command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Configure Hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hive requires a metastore to store metadata about the tables and partitions. We'll use the embedded Derby database for simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.4.1 Create Hive Configuration Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p $HIVE_HOME/conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.4.2 Copy Template Configuration File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cp $HIVE_HOME/conf/hive-default.xml.template $HIVE_HOME/conf/hive-site.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.4.3 Edit `hive-site.xml`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll configure Hive to use the embedded Derby database for the metastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat <<EOL > $HIVE_HOME/conf/hive-site.xml\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
    "<configuration>\n",
    "    <!-- Hive Metastore Database Settings -->\n",
    "    <property>\n",
    "        <name>javax.jdo.option.ConnectionURL</name>\n",
    "        <value>jdbc:derby:;databaseName=metastore_db;create=true</value>\n",
    "        <description>JDBC connect string for a JDBC metastore</description>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>javax.jdo.option.ConnectionDriverName</name>\n",
    "        <value>org.apache.derby.jdbc.EmbeddedDriver</value>\n",
    "        <description>Driver class name for the JDBC metastore</description>\n",
    "    </property>\n",
    "    <!-- Hive Metastore Warehouse Directory -->\n",
    "    <property>\n",
    "        <name>hive.metastore.warehouse.dir</name>\n",
    "        <value>/user/$(whoami)/hive/warehouse</value>\n",
    "        <description>Location of default database for the warehouse</description>\n",
    "    </property>\n",
    "</configuration>\n",
    "EOL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.4.4 Create Warehouse Directory in HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -mkdir -p /user/$(whoami)/hive/warehouse\n",
    "hdfs dfs -chmod g+w /user/$(whoami)/hive/warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Verify Apache Hive Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm that Hive is installed and configured correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hive --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see output similar to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "Hive 3.1.2\n",
    "Subversion git://... (r...)\n",
    "Compiled by ... on ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Run an ETL Process Using Apache Hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll perform an ETL (Extract, Transform, Load) operation using Apache Hive. We'll create a Hive table, load data into it, perform transformations using SQL queries, and store the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Prepare Sample Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the same sample data as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1.1 Create a Sample Data File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't already, create the student_data.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat <<EOL > ~/student_data.txt\n",
    "1,John,Doe,85\n",
    "2,Jane,Smith,92\n",
    "3,Bob,Johnson,76\n",
    "4,Alice,Williams,89\n",
    "5,Tom,Brown,95\n",
    "EOL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1.2 Upload the Data File to HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll copy the student_data.txt file from the local filesystem to HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -mkdir -p /user/$(whoami)/hive_data\n",
    "hdfs dfs -put ~/student_data.txt /user/$(whoami)/hive_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the file in HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -ls /user/$(whoami)/hive_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the student_data.txt file listed in the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Start Hive CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the Hive command-line interface to interact with Hive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will start the Hive shell. From here on, commands prefixed with hive> indicate that they should be run inside the Hive shell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Create a Hive Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create an external table that maps to the data file we uploaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.3.1 Create External Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "hive> CREATE EXTERNAL TABLE student_data(\n",
    "    id INT,\n",
    "    first_name STRING,\n",
    "    last_name STRING,\n",
    "    score INT\n",
    ")\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "STORED AS TEXTFILE\n",
    "LOCATION '/user/$(whoami)/hive_data';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "\n",
    "- CREATE EXTERNAL TABLE: Creates a table without moving the data; the data remains in the specified location.\n",
    "- ROW FORMAT DELIMITED FIELDS TERMINATED BY ',': Specifies that fields are separated by commas.\n",
    "- STORED AS TEXTFILE: Indicates the data is stored in plain text files.\n",
    "- LOCATION: Specifies the directory in HDFS where the data is located."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.3.2 Verify Table Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "hive> SHOW TABLES;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see student_data listed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Run SQL Queries to Transform Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll perform transformations using SQL queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.1 Select All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "hive> SELECT * FROM student_data;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should display all records in the table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.2 Filter Records with Score Greater than 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "hive> CREATE TABLE high_scores AS\n",
    "SELECT * FROM student_data WHERE score > 80;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This query creates a new table high_scores containing students with scores greater than 80."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.3 Calculate Average Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "hive> SELECT AVG(score) AS average_score FROM student_data;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This query calculates the average score of all students."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5 View the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.5.1 Select from `high_scores` Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "hive> SELECT * FROM high_scores;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should display the records of students with high scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.6 Export the Results to HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to export the high_scores table data to an HDFS directory, you can use the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "hive> INSERT OVERWRITE DIRECTORY '/user/$(whoami)/hive_output/high_scores'\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "SELECT * FROM high_scores;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "\n",
    "- INSERT OVERWRITE DIRECTORY: Writes the results of the query to the specified HDFS directory.\n",
    "- ROW FORMAT DELIMITED FIELDS TERMINATED BY ',': Specifies the output format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.7 Exit Hive Shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type the following command to leave the Hive shell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "hive> EXIT;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.8 Verify the Output in HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.8.1 List the Output Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -ls /user/$(whoami)/hive_output/high_scores/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see files containing the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.8.2 Display the Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -cat /user/$(whoami)/hive_output/high_scores/000000_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "2,Jane,Smith,92\n",
    "4,Alice,Williams,89\n",
    "5,Tom,Brown,95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.9 Clean Up (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to remove the tables and output directories:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.9.1 Drop Tables in Hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the Hive shell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "hive> DROP TABLE student_data;\n",
    "hive> DROP TABLE high_scores;\n",
    "hive> EXIT;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.9.2 Remove Output Directories in HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -rm -r /user/$(whoami)/hive_output/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "\n",
    "- Data Preparation: We used the sample dataset representing student records and uploaded it to HDFS.\n",
    "- Creating Tables: We created Hive tables mapped to our data in HDFS.\n",
    "- SQL Queries: We used SQL queries to filter data and perform calculations.\n",
    "- Data Export: We exported query results to HDFS directories.\n",
    "- Hive Shell: Hive provides a command-line interface for executing queries similar to SQL.\n",
    "\n",
    "**Key Concepts**:\n",
    "\n",
    "- Apache Hive: A data warehouse system for Hadoop that facilitates querying and managing large datasets using SQL.\n",
    "- ETL Process: Extracting data from a source, transforming it according to business logic, and loading it into a destination.\n",
    "- HiveQL: Hive's query language, which is similar to SQL and allows for complex data manipulations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: Cleanup and Shutdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this final section, we'll cover how to properly shut down the Hadoop services and clean up any temporary files or directories created during this exercise. Regular cleanup helps maintain system performance and frees up resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Stop Hadoop Services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To gracefully shut down the Hadoop services, we'll stop both HDFS and YARN daemons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.1.1 Stop YARN Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "stop-yarn.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, if the command is not found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "$HADOOP_HOME/sbin/stop-yarn.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected Output\n",
    "\n",
    "You should see messages indicating that the ResourceManager and NodeManager are stopping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "stopping resourcemanager\n",
    "localhost: stopping nodemanager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.1.2 Stop HDFS Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "stop-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "$HADOOP_HOME/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "\n",
    "You should see messages indicating that the NameNode and DataNode are stopping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "Stopping namenodes on [localhost]\n",
    "localhost: stopping namenode\n",
    "localhost: stopping datanode\n",
    "Stopping secondary namenodes [0.0.0.0]\n",
    "0.0.0.0: stopping secondarynamenode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Verify that Services Have Stopped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check that the Hadoop daemons are no longer running by listing Java processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "jps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "\n",
    "You should only see the Jps process listed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "Jps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Clean Up Temp Files (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During Hadoop's operation, temporary files and directories are created. You may choose to clean these up to free disk space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.3.1 Remove HDFS Data Directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to completely remove all data stored in HDFS (including the NameNode and DataNode data), you can delete the Hadoop data directories. By default, these are located in /tmp/hadoop-$(whoami).\n",
    "\n",
    "**Warning**: This will delete all data in HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf /tmp/hadoop-$(whoami)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.3.2 Remove HDFS Files and Directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you only want to remove the files and directories created in HDFS during this exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -rm -r /user/$(whoami)/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4 Remove Sample Data Files (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to remove the sample data files from your local filesystem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm ~/student_data.txt\n",
    "rm ~/etl.pig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.5 Reset Environment Variables (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to remove the environment variables set during this setup:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.5.1 Edit Your Shell Profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open your shell profile file (e.g., ~/.bashrc, ~/.bash_profile, or ~/.zshrc) in a text editor and remove the lines related to HADOOP_HOME, PIG_HOME, and their additions to the PATH.\n",
    "\n",
    "Alternatively, you can use the following commands to remove them automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "sed -i '/# Hadoop Environment Variables/,+2d' ~/.bashrc\n",
    "sed -i '/# Pig Environment Variables/,+2d' ~/.bashrc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The sed command may differ between systems, and the above command works for GNU sed. On macOS, you might need to use sed -i '' instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.6 Restart Your Shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After making changes to your shell profile, restart your terminal or reload the shell configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ~/.bashrc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.7 Additional Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you installed any software specifically for this exercise and wish to remove it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.7.1 Remove Hadoop and Pig Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "sudo rm -rf /usr/local/hadoop\n",
    "sudo rm -rf /usr/local/pig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.7.2 Remove Downloaded Tarballs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm ~/hadoop-*.tar.gz\n",
    "rm ~/pig-*.tar.gz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "\n",
    "- **Stopping Services**: It's important to properly stop Hadoop services to ensure all processes terminate gracefully.\n",
    "- **Cleaning Up**: Removing temporary files and directories helps maintain system health and frees up disk space.\n",
    "- **Environment Variables**: Resetting environment variables cleans up your shell environment if you no longer need Hadoop and Pig commands accessible globally.\n",
    "- **Data Preservation**: Be cautious when deleting data directories to avoid accidental loss of important data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You've completed setting up a basic Hadoop environment and performed ETL operations using Apache Pig. You've learned how to:\n",
    "\n",
    "- Install and configure Hadoop components: HDFS, YARN, and MapReduce.\n",
    "- Start and stop Hadoop services.\n",
    "- Run a sample MapReduce job.\n",
    "- Install and use Apache Pig for data processing.\n",
    "- Clean up your environment after use.\n",
    "\n",
    "This exercise has provided hands-on experience with Hadoop's ecosystem, and you're now better equipped to explore more advanced features and large-scale data processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Steps:\n",
    "\n",
    "- Explore More Pig Scripts: Try modifying the Pig script to perform different transformations or analyses on your data.\n",
    "- Learn Hive: Consider setting up Apache Hive for SQL-like querying of large datasets on Hadoop.\n",
    "- Set Up a Multi-Node Cluster: If you're interested in scaling up, explore setting up a multi-node Hadoop cluster to handle larger workloads."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
